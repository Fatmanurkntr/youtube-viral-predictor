{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad2c63b7",
   "metadata": {},
   "source": [
    "## Adım 1: Setup, Veri Yükleme ve Son Özellik Setini Hazırlama\n",
    "Feature Engineering dosyasında oluşturduğumuz ve en önemli 5 özellikten oluşan o küçültülmüş feature setini (özellik setini) bu dosyaya getirmemiz gerekiyor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8464506b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳ Veri yükleniyor...\n",
      "✅ Veri Hazırlandı ve Eksik Sütunlar Türetildi.\n",
      "Eğitim Seti: 171024 satır\n",
      "Kullanılan Özellikler: ['log_likes', 'log_comment_count', 'dislikes', 'title_length', 'tag_count']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV \n",
    "from xgboost import XGBRegressor \n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# --- 1. VERİ YÜKLEME ---\n",
    "print(\"⏳ Veri yükleniyor...\")\n",
    "try:\n",
    "    df = pd.read_csv('../data/US_youtube_trending_data.csv', encoding='utf-8')\n",
    "except Exception:\n",
    "    df = pd.read_csv('../data/US_youtube_trending_data.csv', encoding='latin1')\n",
    "\n",
    "# Temizlik: Log dönüşümü için 0 olan değerleri atıyoruz\n",
    "df = df[(df['view_count'] > 0) & (df['likes'] > 0)].copy()\n",
    "\n",
    "# --- 2. EKSİK ÖZELLİKLERİ TEKRAR TÜRETME (RE-ENGINEERING) ---\n",
    "# Bu özellikler ham CSV'de olmadığı için burada tekrar yaratmalıyız.\n",
    "\n",
    "# A) Başlık Uzunluğu\n",
    "df['title_length'] = df['title'].str.len()\n",
    "\n",
    "# B) Etiket Sayısı\n",
    "df['tag_count'] = df['tags'].apply(lambda x: 0 if x == '[none]' else len(str(x).split('|')))\n",
    "\n",
    "# --- 3. LOG DÖNÜŞÜMLERİ ---\n",
    "df['log_view_count'] = np.log1p(df['view_count'])\n",
    "df['log_likes'] = np.log1p(df['likes'])\n",
    "df['log_comment_count'] = np.log1p(df['comment_count'])\n",
    "\n",
    "# --- 4. NİHAİ ÖZELLİK SETİNİ SEÇME ---\n",
    "# Feature Selection'da belirlediğimiz en iyi 5 özellik\n",
    "selected_features = [\n",
    "    'log_likes', \n",
    "    'log_comment_count', \n",
    "    'dislikes', \n",
    "    'title_length', \n",
    "    'tag_count'\n",
    "]\n",
    "\n",
    "# Hedef ve Özellikleri Ayırma\n",
    "X = df[selected_features]\n",
    "y = df['log_view_count']\n",
    "\n",
    "# --- 5. VERİ BÖLME (TRAIN / VAL / TEST) ---\n",
    "# Önce Test'i (%20) ayır\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "# Sonra Validation'ı (%20) ayır\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_val, y_train_val, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(\"✅ Veri Hazırlandı ve Eksik Sütunlar Türetildi.\")\n",
    "print(f\"Eğitim Seti: {X_train.shape[0]} satır\")\n",
    "print(f\"Kullanılan Özellikler: {X.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45ee49e",
   "metadata": {},
   "source": [
    "###  Veri Hazırlığı ve Final Özellik Entegrasyonu\n",
    " `3_feature_engineering.ipynb` dosyasından türetilen (`title_length`, `tag_count`) gibi özellikleri bu dosyaya başarıyla yeniden entegre ederek önceki `KeyError` hatalarını gidermiştir. Modelin eğitileceği tüm özellikler ve hedef değişken (`view_count`), doğrusal ilişkiyi sağlamak amacıyla her iki tarafa da **Logaritmik Dönüşüm**den geçirilmiştir. Nihai aşamada, Feature Selection ile belirlenen **5 en güçlü özellik** (Etkileşim ve İçerik Stratejisi) kullanılarak veri, **171.024 satır** ile Train/Validation/Test setlerine ayrılmıştır."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ddadfd",
   "metadata": {},
   "source": [
    "## Adım 2: Model Eğitimi ve Optimizasyon\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35deafdb",
   "metadata": {},
   "source": [
    "###  Algoritma Seçimi:\n",
    "\n",
    "Final model olarak **XGBoost Regressor** tercih edilmiştir. Bu seçim, Baseline kıyaslamasında en yüksek skoru veren **ağaç tabanlı** (Random Forest: 0.6509) model ailesine dayanmaktadır. XGBoost, Random Forest'tan daha hızlı ve daha doğru tahminler yapabilen, endüstri standardı bir algoritmadır. Hedefimiz, bu güçlü algoritmayı kullanarak **0.6509** olan Baseline skoru aşmaktır."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f80b731",
   "metadata": {},
   "source": [
    "### Hiperparametre Optimizasyonu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32008de8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳ GridSearch başlatılıyor... Bu işlem birkaç dakika sürebilir.\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "\n",
      "✅ Optimizasyon Tamamlandı.\n",
      "En İyi Parametreler: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}\n",
      "Optimizasyon Sonrası R2 Skoru (Validation Seti): 0.7802\n"
     ]
    }
   ],
   "source": [
    "# --- HİPERPARAMETRE OPTİMİZASYONU ---\n",
    "print(\"⏳ GridSearch başlatılıyor... Bu işlem birkaç dakika sürebilir.\")\n",
    "\n",
    "# 1. Kıyaslama Yapılacak Parametreler\n",
    "# Optimizasyonun hızını artırmak için basit bir aralık seçiyoruz\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200], # Kaç ağaç kullanılacak?\n",
    "    'max_depth': [3, 5],        # Ağacın derinliği (Komplekslik)\n",
    "    'learning_rate': [0.05, 0.1] # Öğrenme hızı\n",
    "}\n",
    "\n",
    "# 2. XGBoost Modeli\n",
    "xgb_model = XGBRegressor(random_state=42)\n",
    "\n",
    "# 3. GridSearchCV (En iyi parametreyi bul)\n",
    "# Cross-Validation (cv=3) ile denemeler yaparız. n_jobs=-1 tüm çekirdekleri kullanır.\n",
    "grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, \n",
    "                           scoring='r2', cv=3, verbose=1, n_jobs=-1)\n",
    "\n",
    "# 4. Eğitimi Başlat (Validation/Train verisini kullanıyoruz)\n",
    "# Optimizasyonu, Train ve Validation birleşik set üzerinde yapıyoruz.\n",
    "grid_search.fit(X_train_val, y_train_val)\n",
    "\n",
    "print(\"\\n✅ Optimizasyon Tamamlandı.\")\n",
    "print(f\"En İyi Parametreler: {grid_search.best_params_}\")\n",
    "\n",
    "# 5. En İyi Modeli Kaydet ve Skorla\n",
    "best_xgb = grid_search.best_estimator_\n",
    "\n",
    "# Sadece nihai skoru görmek için Tahmin yapalım (Validation Seti üzerinde)\n",
    "y_val_pred = best_xgb.predict(X_val)\n",
    "r2_optimized = r2_score(y_val, y_val_pred)\n",
    "\n",
    "print(f\"Optimizasyon Sonrası R2 Skoru (Validation Seti): {r2_optimized:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df98c8b7",
   "metadata": {},
   "source": [
    "### Optimizasyon Sonuç ve Raporu\n",
    "\n",
    "**1. En İyi Parametreler:** GridSearch, **learning_rate: 0.1, max_depth: 5, n_estimators: 200** parametrelerini en iyi kombinasyon olarak belirlemiştir.\n",
    "\n",
    "**2. R2 Skoru (Nihai Proje Skoru):** **0.7802**\n",
    "\n",
    "**3. Çıkarım:** Optimizasyon sonucunda elde edilen R2 skoru (**0.7802**), Baseline skoru olan **0.6509**'u anlamlı bir farkla aşmıştır. Bu durum, XGBoost algoritması ve seçilen 5 kritik özelliğin (Etkileşim Metrikleri + İçerik Uzunluğu) YouTube verisindeki karmaşık desenleri çözmede başarılı olduğunu kanıtlamaktadır. Modelimiz artık **%78** doğrulukla izlenmeleri tahmin edebilmektedir."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
